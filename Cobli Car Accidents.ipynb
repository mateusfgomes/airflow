{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb92ea8-df5b-48b3-8701-d8df20ac4488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, to_date, year, hour, minute, to_timestamp, lit, concat, expr, regexp_replace\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75a8ffb-2ff2-479e-a607-1a76058422b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(delimiter=';', header='true', infer_schema='true') \\\n",
    "        .load('/Volumes/mateus_dev/default/files/acidentes_brasil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74a761b-5b6f-4fec-ab60-0b771442a84c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n|    id|data_inversa|  dia_semana| horario| uf| br|   km|           municipio|      causa_acidente|       tipo_acidente|classificacao_acidente| fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|    latitude|   longitude|regional|delegacia|           uop|\n+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n|260068|  01/01/2020|quarta-feira|05:40:00| PA|316|   84|SAO FRANCISCO DO ...|Falta de Atencao ...|Saida de leito ca...|                    NA|Pleno dia|Decrescente|             Ceu Claro|   Simples|       Reta|     Nao|      3|     0|            2|             0|     0|        1|      2|       2|  -1,3101929|-47,74456398| SPRF-PA| DEL01-PA|UOP02-DEL01-PA|\n|260073|  01/01/2020|quarta-feira|06:00:00| MG|262|  804|             UBERABA|Falta de Atencao ...| Colisao transversal|   Com Vitimas Feridas|Pleno dia|Decrescente|             Ceu Claro|     Dupla|       Reta|     Sim|      4|     0|            1|             0|     3|        0|      1|       2|-19,76747537|-47,98725511| SPRF-MG| DEL13-MG|UOP01-DEL13-MG|\n|260087|  01/01/2020|quarta-feira|06:00:00| BA|116|  191|             CANUDOS|   Condutor Dormindo|Saida de leito ca...|    Com Vitimas Fatais|Pleno dia|  Crescente|               Nublado|   Simples|       Reta|     Nao|      2|     1|            0|             0|     0|        2|      0|       3|-10,32002103|-39,06425211| SPRF-BA| DEL07-BA|UOP02-DEL07-BA|\n|260116|  01/01/2020|quarta-feira|10:08:00| SP|116|   71|           APARECIDA|Nao guardar dista...|    Colisao traseira|   Com Vitimas Feridas|Pleno dia|  Crescente|                   Sol|     Dupla|       Reta|     Sim|      3|     0|            2|             0|     1|        0|      2|       2|-22,85651665|-45,23114328| SPRF-SP| DEL08-SP|UOP01-DEL08-SP|\n|260129|  01/01/2020|quarta-feira|12:10:00| MG|262|380.9|             JUATUBA|   Condutor Dormindo|Saida de leito ca...|   Com Vitimas Feridas|Pleno dia|  Crescente|             Ceu Claro|     Dupla|      Curva|     Nao|      2|     0|            1|             0|     0|        1|      1|       2|  -19,947864|  -44,381226| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|\n+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2d007f-65fe-4dd0-8e3f-9d6518a56331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n| horario|\n+--------+\n|05:40:00|\n|06:00:00|\n|06:00:00|\n|10:08:00|\n|12:10:00|\n|14:15:00|\n|14:25:00|\n|14:50:00|\n|16:10:00|\n|15:55:00|\n|19:30:00|\n|19:20:00|\n|18:00:00|\n|18:10:00|\n|03:10:00|\n|02:00:00|\n|04:15:00|\n|07:00:00|\n|05:30:00|\n|06:10:00|\n+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.horario).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ff6542-ae68-4851-8704-da76ae2fc1be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"data_inversa\", to_date(col(\"data_inversa\"), \"dd/MM/yyyy\"))\n",
    "df = df.withColumn(\"ano\", year(col(\"data_inversa\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dbc56c-c322-4f67-8fad-afed0af71e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n| horario|\n+--------+\n|05:40:00|\n|06:00:00|\n|06:00:00|\n|10:08:00|\n|12:10:00|\n|14:15:00|\n|14:25:00|\n|14:50:00|\n|16:10:00|\n|15:55:00|\n|19:30:00|\n|19:20:00|\n|18:00:00|\n|18:10:00|\n|03:10:00|\n|02:00:00|\n|04:15:00|\n|07:00:00|\n|05:30:00|\n|06:10:00|\n+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.horario).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a6cdbd-eb04-4bfc-8763-f590669d5505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"/Volumes/mateus_dev/default/files/car_accidents\"\n",
    "\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ano\", \"UF\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d81e87-5f76-4017-a347-741d2a5d071a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['id',\n",
       " 'data_inversa',\n",
       " 'dia_semana',\n",
       " 'horario',\n",
       " 'br',\n",
       " 'km',\n",
       " 'municipio',\n",
       " 'causa_acidente',\n",
       " 'tipo_acidente',\n",
       " 'classificacao_acidente',\n",
       " 'fase_dia',\n",
       " 'sentido_via',\n",
       " 'condicao_metereologica',\n",
       " 'tipo_pista',\n",
       " 'tracado_via',\n",
       " 'uso_solo',\n",
       " 'pessoas',\n",
       " 'mortos',\n",
       " 'feridos_leves',\n",
       " 'feridos_graves',\n",
       " 'ilesos',\n",
       " 'ignorados',\n",
       " 'feridos',\n",
       " 'veiculos',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'regional',\n",
       " 'delegacia',\n",
       " 'uop',\n",
       " 'ano',\n",
       " 'UF']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"/Volumes/mateus_dev/default/files/car_accidents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461e8381-6741-425e-b30e-9f6344b286fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for column in df_parquet.columns:\n",
    "    df_parquet = df_parquet.filter(col(column).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d872296a-5b32-45a8-a93f-b394c3dc5541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = df_parquet.filter(df_parquet.tipo_acidente.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2441a426-2cad-4e7b-94a6-06b9ed7c139d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-700950083878940>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_parquet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoJSON\u001B[49m()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1716\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   1714\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1715\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jdf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jmap\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jcols\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoJSON\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m-> 1716\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m   1717\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m   1718\u001B[0m         )\n",
       "\u001B[1;32m   1719\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\n",
       "\u001B[1;32m   1720\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1721\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocalCheckpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1722\u001B[0m     ]:\n",
       "\u001B[1;32m   1723\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n",
       "\u001B[1;32m   1724\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1725\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n",
       "\u001B[1;32m   1726\u001B[0m         )\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'toJSON' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'toJSON' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "metadata": {
        "errorSummary": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'toJSON' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "JVM_ATTRIBUTE_NOT_SUPPORTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-700950083878940>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_parquet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoJSON\u001B[49m()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1716\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1714\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1715\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jdf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jmap\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jcols\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoJSON\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m-> 1716\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m   1717\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m   1718\u001B[0m         )\n\u001B[1;32m   1719\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[1;32m   1720\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1721\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocalCheckpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1722\u001B[0m     ]:\n\u001B[1;32m   1723\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n\u001B[1;32m   1724\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1725\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   1726\u001B[0m         )\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute 'toJSON' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_parquet.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52eecc77-0cd3-4647-a55d-6e54d13f9394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.filter(df_parquet.tipo_acidente.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b0a42d-6518-4130-8902-d933bd4757f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n| horario|\n+--------+\n|19:10:00|\n|18:30:00|\n|18:30:00|\n|16:40:00|\n|18:00:00|\n|17:30:00|\n|15:00:00|\n|09:35:00|\n|14:50:00|\n|13:15:00|\n|07:40:00|\n|21:40:00|\n|20:00:00|\n|16:10:00|\n|13:45:00|\n|15:50:00|\n|11:00:00|\n|14:30:00|\n|19:10:00|\n|06:00:00|\n+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_parquet.select(df_parquet.horario).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4a6a93-66a1-4d8e-b140-e890e22bfb46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- data_inversa: date (nullable = true)\n |-- dia_semana: string (nullable = true)\n |-- horario: string (nullable = true)\n |-- br: integer (nullable = true)\n |-- km: float (nullable = true)\n |-- municipio: string (nullable = true)\n |-- causa_acidente: string (nullable = true)\n |-- tipo_acidente: string (nullable = true)\n |-- classificacao_acidente: string (nullable = true)\n |-- fase_dia: string (nullable = true)\n |-- sentido_via: string (nullable = true)\n |-- condicao_metereologica: string (nullable = true)\n |-- tipo_pista: string (nullable = true)\n |-- tracado_via: string (nullable = true)\n |-- uso_solo: string (nullable = true)\n |-- pessoas: integer (nullable = true)\n |-- mortos: integer (nullable = true)\n |-- feridos_leves: integer (nullable = true)\n |-- feridos_graves: integer (nullable = true)\n |-- ilesos: integer (nullable = true)\n |-- ignorados: integer (nullable = true)\n |-- feridos: integer (nullable = true)\n |-- veiculos: integer (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- regional: string (nullable = true)\n |-- delegacia: string (nullable = true)\n |-- uop: string (nullable = true)\n |-- ano: integer (nullable = true)\n |-- UF: string (nullable = true)\n |-- data_horario: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_parquet = df_parquet.withColumn(\"id\",df_parquet.id.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"data_inversa\", to_date(col(\"data_inversa\"), \"dd/MM/yyyy\"))\n",
    "df_parquet = df_parquet.withColumn(\"data_horario\", to_timestamp(concat(col(\"data_inversa\"), lit(\" \"), col(\"horario\")), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_parquet = df_parquet.withColumn(\"br\",df_parquet.br.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"km\",df_parquet.km.cast(FloatType()))\n",
    "df_parquet = df_parquet.withColumn(\"pessoas\",df_parquet.pessoas.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"mortos\",df_parquet.mortos.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"feridos_leves\",df_parquet.feridos_leves.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"feridos_graves\",df_parquet.feridos_graves.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"ilesos\",df_parquet.ilesos.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"ignorados\",df_parquet.ignorados.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"feridos\",df_parquet.feridos.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"veiculos\",df_parquet.veiculos.cast(IntegerType()))\n",
    "df_parquet = df_parquet.withColumn(\"latitude\", regexp_replace(col(\"latitude\"), \",\", \".\").cast(DoubleType()))\n",
    "df_parquet = df_parquet.withColumn(\"longitude\", regexp_replace(col(\"longitude\"), \",\", \".\").cast(DoubleType()))\n",
    "df_parquet = df_parquet.withColumn(\"ano\",df_parquet.ano.cast(IntegerType()))\n",
    "\n",
    "df_parquet.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307b27a0-aaea-49c5-b6f1-a1cfc2f1e71e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|       data_horario|\n+-------------------+\n|2020-06-05 19:10:00|\n|2020-06-05 18:30:00|\n|2020-06-05 18:30:00|\n|2020-06-05 16:40:00|\n|2020-06-05 18:00:00|\n+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_parquet.select(df_parquet.data_horario).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6be0a66-333a-4be1-9bd8-a3b5568cf33e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+--------+---+-----+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+----+---+-------------------+\n|    id|data_inversa| dia_semana| horario| br|   km|municipio|      causa_acidente|       tipo_acidente|classificacao_acidente|   fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|    latitude|   longitude|regional|delegacia|           uop| ano| UF|       data_horario|\n+------+------------+-----------+--------+---+-----+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+----+---+-------------------+\n|288337|  2020-06-05|sexta-feira|19:10:00|381|482.8|    BETIM|Falta de Atencao ...|    Colisao traseira|   Com Vitimas Feridas|Plena Noite|  Crescente|             Ceu Claro|  Multipla|    Declive|     Sim|      3|     0|            1|             0|     2|        0|      1|       2|  -19.955532|  -44.097209| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|2020| MG|2020-06-05 19:10:00|\n|288319|  2020-06-05|sexta-feira|18:30:00|267| 65.0|    BICAS|Velocidade Incomp...|Queda de ocupante...|   Com Vitimas Feridas|Plena Noite|  Crescente|               Nublado|   Simples|       Reta|     Nao|      1|     0|            1|             0|     0|        0|      1|       1|-21.73321861|-43.09179399| SPRF-MG| DEL05-MG|UOP01-DEL05-MG|2020| MG|2020-06-05 18:30:00|\n|288314|  2020-06-05|sexta-feira|18:30:00|381|485.0|    BETIM|Defeito Mecanico ...|          Tombamento|   Com Vitimas Feridas|  Pleno dia|  Crescente|             Ceu Claro|     Dupla|      Curva|     Sim|      1|     0|            1|             0|     0|        0|      1|       1|-19.95659361|-44.12622893| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|2020| MG|2020-06-05 18:30:00|\n|288307|  2020-06-05|sexta-feira|16:40:00|116|501.8|  INHAPIM|Falta de Atencao ...| Colisao transversal|   Com Vitimas Feridas|  Pleno dia|  Crescente|             Ceu Claro|   Simples|       Reta|     Nao|      2|     0|            1|             0|     1|        0|      1|       2|-19.57017905|-42.12132627| SPRF-MG| DEL06-MG|UOP02-DEL06-MG|2020| MG|2020-06-05 16:40:00|\n|288304|  2020-06-05|sexta-feira|18:00:00| 40|702.1|BARBACENA|Defeito Mecanico ...|            Incendio|           Sem Vitimas|Plena Noite|  Crescente|               Nublado|     Dupla|       Reta|     Sim|      1|     0|            0|             0|     1|        0|      0|       1|-21.21962096|-43.73897552| SPRF-MG| DEL05-MG|UOP02-DEL05-MG|2020| MG|2020-06-05 18:00:00|\n+------+------------+-----------+--------+---+-----+---------+--------------------+--------------------+----------------------+-----------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+----+---+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25622871-0437-4c5c-9299-d10e7d12f3ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = df_parquet.withColumn(\"percentual_fatalidades\", (df_parquet.mortos/df_parquet.pessoas) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b78f4f7-854e-4717-8f37-c12d7180ac07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.filter(df_parquet.classificacao_acidente == \"NA\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e103b0-868c-443d-a53f-f86a49ee3e2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = df_parquet.withColumn(\"classificacao_acidente\", when(col(\"classificacao_acidente\") == \"NA\", \n",
    "         when(col(\"mortos\") > 0, \"Com Vitimas Fatais\")\n",
    "        .when((col(\"feridos_leves\") + col(\"feridos_graves\")) > 0, \"Com Vitimas Feridas\")\n",
    "        .otherwise(\"Sem Vitimas\"))\n",
    "    .otherwise(col(\"classificacao_acidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a792a5c-e997-446e-8c80-d6f7811539e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|classificacao_acidente|\n+----------------------+\n|    Com Vitimas Fatais|\n|   Com Vitimas Feridas|\n|           Sem Vitimas|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_parquet.select(df_parquet.classificacao_acidente).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b16827-67dc-4984-97ba-ae10ffb0eeba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"ano\").option(\"compression\", \"snappy\").parquet(\"/Volumes/mateus_dev/default/files/silver/car_accidents\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cobli Car Accidents",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
